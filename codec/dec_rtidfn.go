// Copyright (c) 2012-2020 Ugorji Nwoke. All rights reserved.
// Use of this source code is governed by a MIT license found in the LICENSE file.

package codec

import (
	"reflect"
	"sync"
)

func decFnloadFastpathUnderlying[T decDriver](ti *typeInfo, fp *fastpathDs[T]) (f *fastpathD[T], u reflect.Type) {
	rtid := rt2id(ti.fastpathUnderlying)
	idx, ok := fastpathAvIndex(rtid)
	if !ok {
		return
	}
	f = &fp[idx]
	if uint8(reflect.Array) == ti.kind {
		u = reflect.ArrayOf(ti.rt.Len(), ti.elem)
	} else {
		u = f.rt
	}
	return
}

// ----

type decFnInfo struct {
	ti     *typeInfo
	xfFn   Ext
	xfTag  uint64
	addrD  bool
	addrDf bool // force: if addrD, then decode function MUST take a ptr
}

// decFn encapsulates the captured variables and the encode function.
// This way, we only do some calculations one times, and pass to the
// code block that should be called (encapsulated in a function)
// instead of executing the checks every time.
type decFn[D decDriver] struct {
	i  decFnInfo
	fd func(*decoder[D], *decFnInfo, reflect.Value)
	// _  [1]uint64 // padding (cache-aligned)
}

type decRtidFn[D decDriver] struct {
	rtid uintptr
	fn   *decFn[D]
}

// ----

func decFindRtidFn[D decDriver](s []decRtidFn[D], rtid uintptr) (i uint, fn *decFn[D]) {
	// binary search. Adapted from sort/search.go. Use goto (not for loop) to allow inlining.
	var h uint // var h, i uint
	var j = uint(len(s))
LOOP:
	if i < j {
		h = (i + j) >> 1 // avoid overflow when computing h // h = i + (j-i)/2
		if s[h].rtid < rtid {
			i = h + 1
		} else {
			j = h
		}
		goto LOOP
	}
	if i < uint(len(s)) && s[i].rtid == rtid {
		fn = s[i].fn
	}
	return
}

func decFnViaBH[D decDriver](rt reflect.Type, fns *atomicRtidFnSlice, x *BasicHandle, fp *fastpathDs[D],
	checkExt bool) (fn *decFn[D]) {
	return decFnVia[D](rt, fns, x.typeInfos(), &x.mu, x.extHandle, fp,
		checkExt, x.CheckCircularRef, x.timeBuiltin, x.binaryHandle, x.jsonHandle)
}

func decFnVia[D decDriver](rt reflect.Type, fns *atomicRtidFnSlice,
	tinfos *TypeInfos, mu *sync.Mutex, exth extHandle, fp *fastpathDs[D],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *decFn[D]) {
	rtid := rt2id(rt)
	var sp []decRtidFn[D]
	sp = decFromRtidFnSlice[D](fns.load())
	if sp != nil {
		_, fn = decFindRtidFn[D](sp, rtid)
	}
	if fn == nil {
		fn = decFnViaLoader[D](rt, rtid, fns, tinfos, mu, exth, fp, checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json)
	}
	return
}

func decFnViaLoader[D decDriver](rt reflect.Type, rtid uintptr, fns *atomicRtidFnSlice,
	tinfos *TypeInfos, mu *sync.Mutex, exth extHandle, fp *fastpathDs[D],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *decFn[D]) {

	fn = decFnLoad[D](rt, rtid, tinfos, exth, fp, checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json)
	var sp []decRtidFn[D]
	mu.Lock()
	sp = decFromRtidFnSlice[D](fns.load())
	// since this is an atomic load/store, we MUST use a different array each time,
	// else we have a data race when a store is happening simultaneously with a decFindRtidFn call.
	if sp == nil {
		sp = []decRtidFn[D]{{rtid, fn}}
		fns.store(decToRtidFnSlice[D](&sp))
	} else {
		idx, fn2 := decFindRtidFn[D](sp, rtid)
		if fn2 == nil {
			sp2 := make([]decRtidFn[D], len(sp)+1)
			copy(sp2[idx+1:], sp[idx:])
			copy(sp2, sp[:idx])
			sp2[idx] = decRtidFn[D]{rtid, fn}
			fns.store(decToRtidFnSlice[D](&sp2))
		}
	}
	mu.Unlock()
	return
}

func decFnLoad[D decDriver](rt reflect.Type, rtid uintptr, tinfos *TypeInfos,
	exth extHandle, fp *fastpathDs[D],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *decFn[D]) {
	fn = new(decFn[D])
	fi := &(fn.i)
	ti := tinfos.get(rtid, rt)
	fi.ti = ti
	rk := reflect.Kind(ti.kind)

	// anything can be an extension except the built-in ones: time, raw and rawext.
	// ensure we check for these types, then if extension, before checking if
	// it implementes one of the pre-declared interfaces.

	fi.addrDf = true

	if rtid == timeTypId && timeBuiltin {
		fn.fd = (*decoder[D]).kTime
	} else if rtid == rawTypId {
		fn.fd = (*decoder[D]).raw
	} else if rtid == rawExtTypId {
		fn.fd = (*decoder[D]).rawExt
		fi.addrD = true
	} else if xfFn := exth.getExt(rtid, checkExt); xfFn != nil {
		fi.xfTag, fi.xfFn = xfFn.tag, xfFn.ext
		fn.fd = (*decoder[D]).ext
		fi.addrD = true
	} else if (ti.flagSelfer || ti.flagSelferPtr) &&
		!(checkCircularRef && ti.flagSelferViaCodecgen && ti.kind == byte(reflect.Struct)) {
		// do not use Selfer generated by codecgen if it is a struct and CheckCircularRef=true
		fn.fd = (*decoder[D]).selferUnmarshal
		fi.addrD = ti.flagSelferPtr
	} else if supportMarshalInterfaces && binaryEncoding &&
		(ti.flagBinaryMarshaler || ti.flagBinaryMarshalerPtr) &&
		(ti.flagBinaryUnmarshaler || ti.flagBinaryUnmarshalerPtr) {
		fn.fd = (*decoder[D]).binaryUnmarshal
		fi.addrD = ti.flagBinaryUnmarshalerPtr
	} else if supportMarshalInterfaces && !binaryEncoding && json &&
		(ti.flagJsonMarshaler || ti.flagJsonMarshalerPtr) &&
		(ti.flagJsonUnmarshaler || ti.flagJsonUnmarshalerPtr) {
		//If JSON, we should check JSONMarshal before textMarshal
		fn.fd = (*decoder[D]).jsonUnmarshal
		fi.addrD = ti.flagJsonUnmarshalerPtr
	} else if supportMarshalInterfaces && !binaryEncoding &&
		(ti.flagTextMarshaler || ti.flagTextMarshalerPtr) &&
		(ti.flagTextUnmarshaler || ti.flagTextUnmarshalerPtr) {
		fn.fd = (*decoder[D]).textUnmarshal
		fi.addrD = ti.flagTextUnmarshalerPtr
	} else {
		if fastpathEnabled && (rk == reflect.Map || rk == reflect.Slice || rk == reflect.Array) {
			var rtid2 uintptr
			if !ti.flagHasPkgPath { // un-named type (slice or mpa or array)
				rtid2 = rtid
				if rk == reflect.Array {
					rtid2 = rt2id(ti.key) // ti.key for arrays = reflect.SliceOf(ti.elem)
				}
				if idx, ok := fastpathAvIndex(rtid2); ok {
					fn.fd = fp[idx].decfn
					fi.addrD = true
					fi.addrDf = false
					if rk == reflect.Array {
						fi.addrD = false // decode directly into array value (slice made from it)
					}
				}
			} else { // named type (with underlying type of map or slice or array)
				// try to use mapping for underlying type
				xfe, xrt := decFnloadFastpathUnderlying[D](ti, fp)
				if xfe != nil {
					xfnf2 := xfe.decfn
					if rk == reflect.Array {
						fi.addrD = false // decode directly into array value (slice made from it)
						fn.fd = func(d *decoder[D], xf *decFnInfo, xrv reflect.Value) {
							xfnf2(d, xf, rvConvert(xrv, xrt))
						}
					} else {
						fi.addrD = true
						fi.addrDf = false // meaning it can be an address(ptr) or a value
						xptr2rt := reflect.PointerTo(xrt)
						fn.fd = func(d *decoder[D], xf *decFnInfo, xrv reflect.Value) {
							if xrv.Kind() == reflect.Ptr {
								xfnf2(d, xf, rvConvert(xrv, xptr2rt))
							} else {
								xfnf2(d, xf, rvConvert(xrv, xrt))
							}
						}
					}
				}
			}
		}
		if fn.fd == nil {
			switch rk {
			case reflect.Bool:
				fn.fd = (*decoder[D]).kBool
			case reflect.String:
				fn.fd = (*decoder[D]).kString
			case reflect.Int:
				fn.fd = (*decoder[D]).kInt
			case reflect.Int8:
				fn.fd = (*decoder[D]).kInt8
			case reflect.Int16:
				fn.fd = (*decoder[D]).kInt16
			case reflect.Int32:
				fn.fd = (*decoder[D]).kInt32
			case reflect.Int64:
				fn.fd = (*decoder[D]).kInt64
			case reflect.Uint:
				fn.fd = (*decoder[D]).kUint
			case reflect.Uint8:
				fn.fd = (*decoder[D]).kUint8
			case reflect.Uint16:
				fn.fd = (*decoder[D]).kUint16
			case reflect.Uint32:
				fn.fd = (*decoder[D]).kUint32
			case reflect.Uint64:
				fn.fd = (*decoder[D]).kUint64
			case reflect.Uintptr:
				fn.fd = (*decoder[D]).kUintptr
			case reflect.Float32:
				fn.fd = (*decoder[D]).kFloat32
			case reflect.Float64:
				fn.fd = (*decoder[D]).kFloat64
			case reflect.Complex64:
				fn.fd = (*decoder[D]).kComplex64
			case reflect.Complex128:
				fn.fd = (*decoder[D]).kComplex128
			case reflect.Chan:
				fn.fd = (*decoder[D]).kChan
			case reflect.Slice:
				fn.fd = (*decoder[D]).kSlice
			case reflect.Array:
				fi.addrD = false // decode directly into array value (slice made from it)
				fn.fd = (*decoder[D]).kArray
			case reflect.Struct:
				fn.fd = (*decoder[D]).kStruct
			case reflect.Map:
				fn.fd = (*decoder[D]).kMap
			case reflect.Interface:
				// encode: reflect.Interface are handled already by preEncodeValue
				fn.fd = (*decoder[D]).kInterface
			default:
				// reflect.Ptr and reflect.Interface are handled already by preEncodeValue
				fn.fd = (*decoder[D]).kErr
			}
		}
	}
	return
}

// ----
