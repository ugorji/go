// Copyright (c) 2012-2020 Ugorji Nwoke. All rights reserved.
// Use of this source code is governed by a MIT license found in the LICENSE file.

package codec

import (
	"reflect"
	"sync"
)

// ----

func encFnloadFastpathUnderlying[T encDriver](ti *typeInfo, fp *fastpathEs[T]) (f *fastpathE[T], u reflect.Type) {
	var rtid uintptr
	var idx int
	rtid = rt2id(ti.fastpathUnderlying)
	idx = fastpathAvIndex(rtid)
	if idx == -1 {
		return
	}
	f = &fp[idx]
	if uint8(reflect.Array) == ti.kind {
		u = reflect.ArrayOf(ti.rt.Len(), ti.elem)
	} else {
		u = f.rt
	}
	return
}

// ----

type encFnInfo struct {
	ti    *typeInfo
	xfFn  Ext
	xfTag uint64
	addrE bool
	// addrEf bool // force: if addrE, then encode function MUST take a ptr
}

// encFn encapsulates the captured variables and the encode function.
// This way, we only do some calculations one times, and pass to the
// code block that should be called (encapsulated in a function)
// instead of executing the checks every time.
type encFn[E encDriver] struct {
	i  encFnInfo
	fe func(*encoder[E], *encFnInfo, reflect.Value)
	// _  [1]uint64 // padding (cache-aligned)
}

type encRtidFn[E encDriver] struct {
	rtid uintptr
	fn   *encFn[E]
}

// ----

func encFindRtidFn[E encDriver](s []encRtidFn[E], rtid uintptr) (i uint, fn *encFn[E]) {
	// binary search. Adapted from sort/search.go. Use goto (not for loop) to allow inlining.
	var h uint // var h, i uint
	var j = uint(len(s))
LOOP:
	if i < j {
		h = (i + j) >> 1 // avoid overflow when computing h // h = i + (j-i)/2
		if s[h].rtid < rtid {
			i = h + 1
		} else {
			j = h
		}
		goto LOOP
	}
	if i < uint(len(s)) && s[i].rtid == rtid {
		fn = s[i].fn
	}
	return
}

func encFnViaBH[E encDriver](rt reflect.Type, fns *atomicRtidFnSlice,
	x *BasicHandle, fp *fastpathEs[E], checkExt bool) (fn *encFn[E]) {
	return encFnVia[E](rt, fns, x.typeInfos(), &x.mu, x.extHandle, fp,
		checkExt, x.CheckCircularRef, x.timeBuiltin, x.binaryHandle, x.jsonHandle)
}

func encFnVia[E encDriver](rt reflect.Type, fns *atomicRtidFnSlice,
	tinfos *TypeInfos, mu *sync.Mutex, exth extHandle, fp *fastpathEs[E],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *encFn[E]) {
	rtid := rt2id(rt)
	var sp []encRtidFn[E]
	sp = encFromRtidFnSlice[E](fns.load())
	if sp != nil {
		_, fn = encFindRtidFn[E](sp, rtid)
	}
	if fn == nil {
		fn = encFnViaLoader[E](rt, rtid, fns, tinfos, mu, exth, fp, checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json)
	}
	return
}

func encFnViaLoader[E encDriver](rt reflect.Type, rtid uintptr, fns *atomicRtidFnSlice,
	tinfos *TypeInfos, mu *sync.Mutex, exth extHandle, fp *fastpathEs[E],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *encFn[E]) {

	fn = encFnLoad[E](rt, rtid, tinfos, exth, fp, checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json)
	var sp []encRtidFn[E]
	mu.Lock()
	sp = encFromRtidFnSlice[E](fns.load())
	// since this is an atomic load/store, we MUST use a different array each time,
	// else we have a data race when a store is happening simultaneously with a encFindRtidFn call.
	if sp == nil {
		sp = []encRtidFn[E]{{rtid, fn}}
		fns.store(encToRtidFnSlice[E](&sp))
	} else {
		idx, fn2 := encFindRtidFn[E](sp, rtid)
		if fn2 == nil {
			sp2 := make([]encRtidFn[E], len(sp)+1)
			copy(sp2[idx+1:], sp[idx:])
			copy(sp2, sp[:idx])
			sp2[idx] = encRtidFn[E]{rtid, fn}
			fns.store(encToRtidFnSlice[E](&sp2))
		}
	}
	mu.Unlock()
	return
}

func encFnLoad[E encDriver](rt reflect.Type, rtid uintptr, tinfos *TypeInfos,
	exth extHandle, fp *fastpathEs[E],
	checkExt, checkCircularRef, timeBuiltin, binaryEncoding, json bool) (fn *encFn[E]) {
	fn = new(encFn[E])
	fi := &(fn.i)
	ti := tinfos.get(rtid, rt)
	fi.ti = ti
	rk := reflect.Kind(ti.kind)

	// anything can be an extension except the built-in ones: time, raw and rawext.
	// ensure we check for these types, then if extension, before checking if
	// it implementes one of the pre-declared interfaces.

	// fi.addrEf = true

	if rtid == timeTypId && timeBuiltin {
		fn.fe = (*encoder[E]).kTime
	} else if rtid == rawTypId {
		fn.fe = (*encoder[E]).raw
	} else if rtid == rawExtTypId {
		fn.fe = (*encoder[E]).rawExt
		fi.addrE = true
	} else if xfFn := exth.getExt(rtid, checkExt); xfFn != nil {
		fi.xfTag, fi.xfFn = xfFn.tag, xfFn.ext
		fn.fe = (*encoder[E]).ext
		if rk == reflect.Struct || rk == reflect.Array {
			fi.addrE = true
		}
	} else if (ti.flagSelfer || ti.flagSelferPtr) &&
		!(checkCircularRef && ti.flagSelferViaCodecgen && ti.kind == byte(reflect.Struct)) {
		// do not use Selfer generated by codecgen if it is a struct and CheckCircularRef=true
		fn.fe = (*encoder[E]).selferMarshal
		fi.addrE = ti.flagSelferPtr
	} else if supportMarshalInterfaces && binaryEncoding &&
		(ti.flagBinaryMarshaler || ti.flagBinaryMarshalerPtr) &&
		(ti.flagBinaryUnmarshaler || ti.flagBinaryUnmarshalerPtr) {
		fn.fe = (*encoder[E]).binaryMarshal
		fi.addrE = ti.flagBinaryMarshalerPtr
	} else if supportMarshalInterfaces && !binaryEncoding && json &&
		(ti.flagJsonMarshaler || ti.flagJsonMarshalerPtr) &&
		(ti.flagJsonUnmarshaler || ti.flagJsonUnmarshalerPtr) {
		//If JSON, we should check JSONMarshal before textMarshal
		fn.fe = (*encoder[E]).jsonMarshal
		fi.addrE = ti.flagJsonMarshalerPtr
	} else if supportMarshalInterfaces && !binaryEncoding &&
		(ti.flagTextMarshaler || ti.flagTextMarshalerPtr) &&
		(ti.flagTextUnmarshaler || ti.flagTextUnmarshalerPtr) {
		fn.fe = (*encoder[E]).textMarshal
		fi.addrE = ti.flagTextMarshalerPtr
	} else {
		if fastpathEnabled && (rk == reflect.Map || rk == reflect.Slice || rk == reflect.Array) {
			// by default (without using unsafe),
			// if an array is not addressable, converting from an array to a slice
			// requires an allocation (see helper_not_unsafe.go: func rvGetSlice4Array).
			//
			// (Non-addressable arrays mostly occur as keys/values from a map).
			//
			// However, fastpath functions are mostly for slices of numbers or strings,
			// which are small by definition and thus allocation should be fast/cheap in time.
			//
			// Consequently, the value of doing this quick allocation to elide the overhead cost of
			// non-optimized (not-unsafe) reflection is a fair price.
			var rtid2 uintptr
			if !ti.flagHasPkgPath { // un-named type (slice or mpa or array)
				rtid2 = rtid
				if rk == reflect.Array {
					rtid2 = rt2id(ti.key) // ti.key for arrays = reflect.SliceOf(ti.elem)
				}
				if idx := fastpathAvIndex(rtid2); idx != -1 {
					fn.fe = fp[idx].encfn
				}
			} else { // named type (with underlying type of map or slice or array)
				// try to use mapping for underlying type
				xfe, xrt := encFnloadFastpathUnderlying[E](ti, fp)
				if xfe != nil {
					xfnf := xfe.encfn
					fn.fe = func(e *encoder[E], xf *encFnInfo, xrv reflect.Value) {
						xfnf(e, xf, rvConvert(xrv, xrt))
					}
				}
			}
		}
		if fn.fe == nil {
			switch rk {
			case reflect.Bool:
				fn.fe = (*encoder[E]).kBool
			case reflect.String:
				// Do not use different functions based on StringToRaw option, as that will statically
				// set the function for a string type, and if the Handle is modified thereafter,
				// behaviour is non-deterministic
				// i.e. DO NOT DO:
				//   if x.StringToRaw {
				//   	fn.fe = (*encoder[E]).kStringToRaw
				//   } else {
				//   	fn.fe = (*encoder[E]).kStringEnc
				//   }

				fn.fe = (*encoder[E]).kString
			case reflect.Int:
				fn.fe = (*encoder[E]).kInt
			case reflect.Int8:
				fn.fe = (*encoder[E]).kInt8
			case reflect.Int16:
				fn.fe = (*encoder[E]).kInt16
			case reflect.Int32:
				fn.fe = (*encoder[E]).kInt32
			case reflect.Int64:
				fn.fe = (*encoder[E]).kInt64
			case reflect.Uint:
				fn.fe = (*encoder[E]).kUint
			case reflect.Uint8:
				fn.fe = (*encoder[E]).kUint8
			case reflect.Uint16:
				fn.fe = (*encoder[E]).kUint16
			case reflect.Uint32:
				fn.fe = (*encoder[E]).kUint32
			case reflect.Uint64:
				fn.fe = (*encoder[E]).kUint64
			case reflect.Uintptr:
				fn.fe = (*encoder[E]).kUintptr
			case reflect.Float32:
				fn.fe = (*encoder[E]).kFloat32
			case reflect.Float64:
				fn.fe = (*encoder[E]).kFloat64
			case reflect.Complex64:
				fn.fe = (*encoder[E]).kComplex64
			case reflect.Complex128:
				fn.fe = (*encoder[E]).kComplex128
			case reflect.Chan:
				fn.fe = (*encoder[E]).kChan
			case reflect.Slice:
				fn.fe = (*encoder[E]).kSlice
			case reflect.Array:
				fn.fe = (*encoder[E]).kArray
			case reflect.Struct:
				if ti.anyOmitEmpty ||
					ti.flagMissingFielder ||
					ti.flagMissingFielderPtr {
					fn.fe = (*encoder[E]).kStruct
				} else {
					fn.fe = (*encoder[E]).kStructNoOmitempty
				}
			case reflect.Map:
				fn.fe = (*encoder[E]).kMap
			case reflect.Interface:
				// encode: reflect.Interface are handled already by preEncodeValue
				fn.fe = (*encoder[E]).kErr
			default:
				// reflect.Ptr and reflect.Interface are handled already by preEncodeValue
				fn.fe = (*encoder[E]).kErr
			}
		}
	}
	return
}

// ----
